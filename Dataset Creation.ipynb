{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc769a2",
   "metadata": {},
   "source": [
    "## 1. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc91d9",
   "metadata": {},
   "source": [
    "This notebook should serve as a guide to the creation of your Carnatic Music Instrument dataset. We will start with the loading of the dataset using the mirdata API, extract the relevant sections and instruments, apply any relevant processing steps, and store the dataset in an intuitive and accessible format.\n",
    "\n",
    "Typical Carnatic Music ensembles contain a wide-range of instruments. For this task we are going to focus on:\n",
    "\n",
    "- Voice\n",
    "- Violin\n",
    "- Mridangam\n",
    "\n",
    "You can refer to the instrumentation section of the [compIAM tutorial](https://mtg.github.io/IAM-tutorial-ismir22/indian_art_music/carnatic-music.html) for more information.\n",
    "\n",
    "The final dataset will be a collection of short audios corresponding to each of these instruments. They will be organised such that each can be retrieved according to the instrument they contain, the performer, the raga and a unique identifier (for reproducibility later).\n",
    "\n",
    "It is up to you to fill in each subsection with the relevant code to perform that task. If possible, try and split the sections amongst the project group to work in parallel. When the task is complete, you should try and abstract the code into .py files so that it can be ran without a python notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee5df1",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2399dd0",
   "metadata": {},
   "source": [
    "You can access the Saraga Carnatic dataset using the [mirdata API](https://github.com/mir-dataset-loaders/mirdata). You should already have the dataset downloaded on your machine in the mirdata repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6020868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mirdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3811c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_home = '/Volumes/MyPassport/mir_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "saraga = mirdata.initialize('saraga_carnatic', data_home=data_home)\n",
    "saraga.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099dc97",
   "metadata": {},
   "source": [
    "You can choose a random track using `.choice_track()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_track = saraga.choice_track()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5c8cd",
   "metadata": {},
   "source": [
    "Explore the metadata for this example track. What information is available? Do all tracks have multi track recordings (i.e. separate voice, violin and mridangam audios)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa81a8",
   "metadata": {},
   "source": [
    "Maybe you have noticed that each track has a unique id - the track_id. You can list all available track_ids using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659345d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tracks = saraga.load_tracks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-university",
   "metadata": {},
   "source": [
    "Can you create some functions to explore these tracks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return a dataframe of associated metadata\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    return metadata\n",
    "\n",
    "def get_performer(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the performer\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    return performer\n",
    "\n",
    "def get_performance(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the performance name\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    return performance\n",
    "\n",
    "def get_raga(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the raga name\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    return raga\n",
    "\n",
    "\n",
    "def get_tonic(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the tonic in hertz\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    return tonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-battlefield",
   "metadata": {},
   "source": [
    "How many ragas/performers/performances are available?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9756db3",
   "metadata": {},
   "source": [
    "### Load Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-auction",
   "metadata": {},
   "source": [
    "The mirdata API returns paths to audio files associated with each track. Can you create some loaders to load an audio based on a given track name. Hint: The `librosa` library contains functions to load audio from file to an array of amplitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mixed_audio(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the loaded audio\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    return audio_array\n",
    "\n",
    "def load_violin_audio(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the isolated violin track\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    return audio_array\n",
    "\n",
    "def load_voice_audio(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the isolated voice track\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    return audio_array\n",
    "\n",
    "def load_mridangam_audio(track_id):\n",
    "    \"\"\"\n",
    "    For <track_id>, return the isolated mridangam track\n",
    "    \"\"\"\n",
    "    # code here\n",
    "    return audio_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-hours",
   "metadata": {},
   "source": [
    "### Listen to Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-beverage",
   "metadata": {},
   "source": [
    "Let's write some functions to listen and visualise these audio arrays in the notebook. You should find that the `IPython` and `matplotlib` library useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_waveform(audio_array):\n",
    "    \"\"\"\n",
    "    Plot waveform for <audio_array> using matplotlib.pyplot\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def play_audio(audio_array):\n",
    "    \"\"\"\n",
    "    Generate audio player for <audio_array> using Ipython library\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-cornell",
   "metadata": {},
   "source": [
    "Are there any important observations about the mixed or isolated instrument tracks? What is the quality like, do you here all of the instruments clearly?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-tolerance",
   "metadata": {},
   "source": [
    "What about if you wanted to plot or listen to just a sample of the audio track?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-currency",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-lease",
   "metadata": {},
   "source": [
    "Are the isolated voccal tracks sufficiently isolated? Libraries like `spleeter` can help separate singing sources from background instruments. Does it help here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_voice(audio):\n",
    "    \"\"\"\n",
    "    Apply spleeter source separation to input audio\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-forum",
   "metadata": {},
   "source": [
    "How does the quality compare? Does spleeter work effectively? Do we lose any important information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-november",
   "metadata": {},
   "source": [
    "### Tagging Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-genius",
   "metadata": {},
   "source": [
    "We want to tag our audios with whether or not a particular instrument is sounding. We can do this by identifying non-silent regions in the isolated tracks and tagging the mixed tracks with the instrument. The `librosa` library contains functionality for identifying silent regions in audio (`librosa.effects.split`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_silence(audio_array):\n",
    "    \"\"\"\n",
    "    Return array of 0 and 1 (is silent/is not silent) for input <audio_array>\n",
    "    \"\"\"\n",
    "    return is_silent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-parish",
   "metadata": {},
   "source": [
    "Do these regions correspond to what you hear when playing the audio with `play_audio()` or what you see with `plot_waveform`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-accommodation",
   "metadata": {},
   "source": [
    "Perhaps very small silent gaps can be ignored as either errors or momentary pauses inherent in the performance of that instrument (such as inhaling with the sung voice). Can you interpolate these small gaps below some threshold? What is a suitable threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_silence(is_silent, threshold=0.5):\n",
    "    \"\"\"\n",
    "    For an input array or 0/1 values (silent/is not silent), fill small gaps of less than <threshold>.\n",
    "    \"\"\"\n",
    "    return is_silent_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-kitchen",
   "metadata": {},
   "source": [
    "Can we tag our mixed tracks with instrument onsets using silence in their constituent isolated tracks as a proxy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_mix(track_id):\n",
    "    \"\"\"\n",
    "    For track_id return three arrays corresponding to violin, mridangam and voice onsets.\n",
    "    \n",
    "    Return violin_onsets, mridangam_onsets, voice_onsets\n",
    "        (each an array of 0 and 1 values: is violin/mridangam/voice sounding or no?)\n",
    "    \"\"\"\n",
    "    return violin_onsets, mridangam_onsets, voice_onsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-venue",
   "metadata": {},
   "source": [
    "### Extracting Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-court",
   "metadata": {},
   "source": [
    "We should now have all the tools necessary to load and annotated audio. We now want to extract small snippets of audio  from the mixed tracks across the dataset and annotate each of these snippets as either containing voice, mridangam, violin or none of the above (a single audio should be able to have more than one tag). \n",
    "\n",
    "It is important that we have examples for all combinations of tags (violin, voice, mridangam, none). Each sample should be of the same length (what should that length be? think about the two extreme cases of very very short and very long, what problems would arise in each of these cases).\n",
    "\n",
    "Each sample should have a unique identifier (index). The information relating to their tags should be stored in a metadata DataFrame where you can also find information about the performance.\n",
    "\n",
    "These should all be saved in individual audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata JSON\n",
    "# extract samples\n",
    "# annotate\n",
    "# save samples with unique index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-validation",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-ocean",
   "metadata": {},
   "source": [
    "With our dataset created and saved in an intuitive and accessible format. Let's create some loaders to load the files and get metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample(index):\n",
    "    \"\"\"\n",
    "    Load sample with index, <index>\n",
    "    \"\"\"\n",
    "    return sample\n",
    "\n",
    "def get_metadata(index):\n",
    "    \"\"\"\n",
    "    Get metadata for sample with index, <index>\n",
    "    \"\"\"\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-concert",
   "metadata": {},
   "source": [
    "Typically, when datasets are presented, they are accompanied by some stats detailing their size and constiuent parts. What stats can you tell us about our dataset? Think about: number of seconds, performers, performances, instruments, ragas, filesizes etc.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-match",
   "metadata": {},
   "source": [
    "### Reproducible Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-chicago",
   "metadata": {},
   "source": [
    "Jupyter notebooks are great for experimenting, especially when visualisation or audio playback is required. However they are not great for reproducibility or source control. Can you abstract the code created here to .py file(s) so that the code can be ran in future without having to load the HTML notebook?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carnatic",
   "language": "python",
   "name": "carnatic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
